# Trabajo Práctico Final

## 1. Dataset y Preprocesamiento
- ¿Por qué es necesario redimensionar las imágenes a un tamaño fijo para una MLP?
Una MLP (MultiLayer Perceptron) no trabaja con imágenes sino con vectores unidimensionales, por lo que cada imagen debe convertirse a un vector de tamaño fijo para ingresar a la red. El tamaño de los vectores depende del tamaño de las imágenes: la cantidad de valores que tiene el vector es igual a la multiplicación entre la cantidad de canales, el ancho y el alto de la imagen. Si se utilizaran imágenes con tamaños distintos, los vectores resultantes tendrían diferente cantidad de valores, y esto generaría incompatibilidad con la MLP. Como la MLP define sus pesos en función del tamaño de la entrada, ingresar más o menos valores de los esperados haría que la red no funcione.
- ¿Qué ventajas ofrece Albumentations frente a otras librerías de transformación como `torchvision.transforms`?
Albumentations es una librería que se usa para aumentar artificialmente la cantidad y la diversidad de datos de entrenamiento generando copias modificadas de datos existentes. A diferencia de torchvision, que utiliza tensores de PyTorch, Albumentations opera sobre arrays de Numpy. Esto permite que las transformaciones sean más rápidas y eficientes. A su vez, ofrece más parámetros para ajustar las transformaciones (como probabilidad, intensidad y modos de relleno); y proporciona un mejor manejo de bounding boxes y keypoints. También, presenta un soporte integrado para la aumentación de máscaras al operar sobre imágenes: alinea máscaras con transformaciones geométricas y mantiene la integridad de los valores de las máscaras.
- ¿Qué hace `A.Normalize()`? ¿Por qué es importante antes de entrenar una red?
La transformación A.Normalize() se utiliza para normalizar los valores de los píxeles de cada canal de una imagen, restando el valor medio del canal y dividiendo por la desviación estándar. Se aplica tanto a las imágenes de entrenamiento como a las de validación. 
Normalizar las entradas es importante porque permite reducir la probabilidad de saturación en las funciones de activación que podrían llevar al gradiente a valores extremos, generando una actualización inestable de pesos. En consecuencia, el entrenamiento se vuelve más estable y el descenso por gradiente es más eficiente, reduciendo las oscilaciones y permitiendo que la función de pérdida (loss) disminuya de forma más rápida. De esta manera, aumenta la velocidad de convergencia. 
- ¿Por qué convertimos las imágenes a `ToTensorV2()` al final de la pipeline?
Para la definición del modelo MLPClassifier se utiliza el módulo torch de la librería PyTorch. Esta librería recibe y trabaja con tensores, mientras que Albumentations opera sobre arrays de Numpy. Por este motivo, una vez que se aplican las transformaciones a las imágenes de entrada (entrenamiento y validación) con Albumentations, es esencial convertirlas en tensores (multidimensionales, en una primera instancia) para que puedan ser procesadas por el modelo. 

## 2. Arquitectura del Modelo
- ¿Por qué usamos una red MLP en lugar de una CNN aquí? ¿Qué limitaciones tiene?
Utilizamos una red MLP porque es fácil de implementar e interpretar dado que consiste en capas conectadas que reciben vectores de entrada y producen una salida para cada clase. De esta manera, se facilita la comprensión de cómo fluye la información desde la obtención de las imágenes, hasta la definición del modelo de clasificación y su respectiva evaluación.  
Sin embargo, el uso de una MLP para clasificar imágenes introduce limitaciones. En primer lugar, su implementación requiere aplanar las imágenes para convertirlas en vectores. En una imagen RGB, los tres canales (R, G y B) se convierten en un único vector largo, compuesto por una sección de valores rojos, otra de verdes y otra de azules. Este proceso elimina la distribución espacial de la información: píxeles que originalmente estaban cerca, dejan de estar relacionados entre sí en la entrada del modelo. Como consecuencia, se pierde la capacidad de reconocer patrones locales como formas, bordes o texturas. 
Por otro lado, en una red totalmente conectada, los pesos son dependientes de la posición exacta de los píxeles en el vector. Esto significa que, si un objeto aparece a la izquierda o a la derecha de la imagen, el patrón activará pesos completamente distintos. Así, el modelo no puede generalizar adecuadamente porque aprende únicamente la configuración espacial específica que ve durante el entrenamiento. Para que aprenda, debería exponerse a múltiples versiones de una misma imagen, desplazando al objeto en distintas direcciones. Esto resultaría poco eficiente. 
- ¿Qué hace la capa `Flatten()` al principio de la red?
La capa Flatten() aplana a las imágenes, es decir, permite convertir a los tensores multidimensionales en vectores unidimensionales, considerando que las capas lineales de la red MLP solo aceptan vectores en 1D. El tamaño (cantidad de valores) de los vectores resulta de la multiplicación de la cantidad de canales y la resolución de las imágenes. De esta manera, en este caso, se tienen vectores de 12.288 valores (64 x 64 x 3). 
- ¿Qué función de activación se usó? ¿Por qué no usamos `Sigmoid` o `Tanh`?
En el pipeline, se utilizó la función de activación ReLU. Las funciones Sigmoid y Tanh no se usaron, en primer lugar, porque implican un mayor costo computacional a la hora de trabajar con tensores grandes, considerando que se basan en funciones exponenciales. 
Por otro lado, Sigmoid y Tanh tienden a saturarse cuando sus entradas son valores muy grandes o muy pequeños. En este caso, las entradas de la MLP son imágenes aplanadas de 12.288 valores (64 x 64 x 3). En la primera capa lineal, cada neurona realiza una suma de estos 12.288 valores multiplicados por sus pesos y agregando un sesgo. Esta operación puede generar valores relativamente grandes o pequeños, lo que podría saturar a Sigmoid o Tanh. En zonas saturadas, la derivada de la función de activación es casi cero; como consecuencia, el gradiente también se vuelve prácticamente nulo y los pesos se actualizan muy lentamente o no se actualizan (vanishing gradient). Este mismo comportamiento puede repetirse en las capas ocultas siguientes, aunque tengan menos entradas.
- ¿Qué parámetro del modelo deberíamos cambiar si aumentamos el tamaño de entrada de la imagen?
Al aumentar el tamaño de entrada de la imagen, debe modificarse el input_size utilizado en la primera capa lineal. Esto se debe a que cada neurona de esa capa tiene un peso asociado a cada valor de entrada, siendo que se necesitan tantos pesos como valores de entrada. Las capas siguientes no se ven afectadas. 

## 3. Entrenamiento y Optimización
- ¿Qué hace `optimizer.zero_grad()`?
El optimizer.zero_grad() se utiliza para resetear el cálculo del gradiente para cada batch. De esta forma, evita que se acumulen los valores calculados anteriormente para que la actualización de pesos refleje solo el batch actual.
- ¿Por qué usamos `CrossEntropyLoss()` en este caso?
Usamos CrossEntropyLoss() porque estamos resolviendo un problema de clasificación multiclase, donde cada imagen pertenece a una sola clase. La última capa de la red MLP produce un score que representa la preferencia de la red por cada clase, pero que no es una probabilidad. Entonces, CrossEntropyLoss() aplica softmax para convertir los scores en probabilidades (asociadas con la pertenencia de la imagen a cada clase) y luego, calcula la loss logarítmica; que se refiere a qué tan lejos está cada predicción de la clase correcta. Estos valores guían la actualización de los pesos durante el entrenamiento.
- ¿Cómo afecta la elección del tamaño de batch (`batch_size`) al entrenamiento?
El tamaño del batch influye en la estabilidad y actualización de pesos durante el entrenamiento. Para épocas pequeñas, el entrenamiento suele ser más lento porque se necesita una mayor cantidad de iteraciones para procesar toda la base de datos. Asimismo, el gradiente resulta muy ruidoso y por ende, inestable. Por el contrario, si los batches tienen mayor tamaño, el gradiente es más estable y preciso, aunque la capacidad de escapar de mínimos locales disminuye. Además, el modelo tiende a ser más determinista (por la gran cantidad de datos considerados), lo que puede llevar al overfitting. 
- ¿Qué pasaría si no usamos `model.eval()` durante la validación?
En el pipeline, model.eval() se utiliza antes de realizar la validación para cambiar el modelo al modo de evaluación. Esto resulta importante porque indica explícitamente que el modelo está en modo de inferencia y evita errores si se agregan capas cuyo comportamiento depende del modo de ejecución. Sin embargo, como en este caso no se usa BatchNorm ni Dropout (que dependen del modo de ejecución), el resultado de la validación para este modelo no mostraría cambios significativos.  Aun así, incluirlo es una buena práctica y prepara el código para posibles ampliaciones futuras.

## 4. Validación y Evaluación
- ¿Qué significa una accuracy del 70% en validación pero 90% en entrenamiento?
El accuracy en entrenamiento suele ser una cota para el accuracy en validación. Sin embargo, en este caso, la respuesta en validación resulta significativamente menor a la de entrenamiento, lo cual podría indicar overfitting. Esto quiere decir que el modelo aprende demasiado bien los datos de entrenamiento, pero no logra generalizar a ejemplos nuevos.
- ¿Qué otras métricas podrían ser más relevantes que accuracy en un problema real?
Algunas de las métricas que deberían considerarse son el F1-Score, precision, recall y matriz de confusión.
- ¿Qué información útil nos da una matriz de confusión que no nos da la accuracy?
El accuracy se utiliza para medir el porcentaje de entradas clasificadas correctamente con respecto al total. En cambio, la matriz de confusión permite comparar los valores predecidos con los reales para poder determinar qué clases se confunden entre sí en la clasificación.
- En el reporte de clasificación, ¿qué representan `precision`, `recall` y `f1-score`?
Precision indica qué proporción de las predicciones positivas de una clase fueron correctas; es decir, mide cuántas de las veces que el modelo dijo que una imagen era de una clase, realmente lo era. Recall mide qué proporción de los ejemplos reales de una clase fueron correctamente identificados por el modelo. F1-score es  una medida del rendimiento general del modelo, la cual considera tanto los falsos positivos (precision) como los falsos negativos (recall). Solo es alto si la precisión y el recall lo son. 

## 5. TensorBoard y Logging
- ¿Qué ventajas tiene usar TensorBoard durante el entrenamiento?
El uso de TensorBoard durante el entrenamiento permite visualizar en tiempo real la evolución de métricas (en este caso, loss y accuracy), lo cual facilita la detección de problemas como overfitting, underfitting o un aprendizaje demasiado lento. Además, proporciona gráficos, textos e imágenes que ayudan a interpretar mejor del proceso de entrenamiento. 
- ¿Qué diferencias hay entre loguear `add_scalar`, `add_image` y `add_text`?
En primer lugar, add_scalar se utiliza para registrar valores numéricos, como loss, accuracy o cualquier métrica que cambie con los batches. Así, permiten generar curvas que sirven para analizar el desempeño del modelo. Por otro lado, add_image se emplea para guardar imágenes. Asimismo, es útil para visualizar datos de entrada o máscaras de segmentación. Finalmente, add_text se usa para almacenar información textual, como el classification report, lo que facilita ver resultados detallados dentro de TensorBoard sin abrir archivos externos. 
- ¿Por qué es útil guardar visualmente las imágenes de validación en TensorBoard?
Guardar visualmente las imágenes permite analizar si el dataset está correctamente cargado y preprocesado. Al observarlas, se puede verificar que las normalizaciones son las correctas, que no existen imágenes mal etiquetadas y que las resoluciones son consistentes. Así, podrían detectarse problemas que no pueden evaluarse a partir de las métricas. 
- ¿Cómo se puede comparar el desempeño de distintos experimentos en TensorBoard?
Para comparar el desempeño de distintos experimentos en TensorBoard, debe registrarse cada experimento con sus respectivos logs en carpetas distintas dentro del directorio general de runs. Luego, al ejecutar TensorBoard, la herramienta detecta automáticamente todas las sesiones y permite visualizar sus características superpuestas. Esto sirve para comparar curvas de pérdida (loss), accuracy, histogramas o imágenes registradas; y entonces, analizar el rendimiento de cada ejecución. 

## 6. Generalización y Transferencia
- ¿Qué cambios habría que hacer si quisiéramos aplicar este mismo modelo a un dataset con 100 clases?
Inicialmente, debería modificarse el número de clases (num_classes) a 100 en la última capa, para que esta genere 100 scores (uno por clase). Además, podría considerarse aumentar el número de capas ocultas (ampliar la red); e incrementar el tamaño de los batches, para que cada época tenga mayor diversidad de clases y el entrenamiento sea más estable. 
- ¿Por qué una CNN suele ser más adecuada que una MLP para clasificación de imágenes?
Una CNN es más adecuada para la clasificación de imágenes porque considera la distribución espacial de los datos: distingue patrones particulares locales, independientemente de su posición, para poder aprender otras estructuras más complejas. A su vez, tienen menos parámetros porque la definición de sus parámetros no depende de la resolución de las imágenes de entrada, sino del del tamaño del kernel (neurona), de la cantidad de filtros y de la cantidad de canales.
- ¿Qué problema podríamos tener si entrenamos este modelo con muy pocas imágenes por clase?
El principal problema de tener muy pocas imágenes por clase es el overfitting: el modelo memorizaría las imágenes en lugar de aprender patrones generales. Esto se traduciría en un alto accuracy durante el entrenamiento pero un bajo accuracy en la validación. 
- ¿Cómo podríamos adaptar este pipeline para imágenes en escala de grises?
Para adaptar el pipeline para imágenes en escala de grises, en primer lugar, debe modificarse la lectura de las imágenes de img = Image.open(path).convert("RGB") a img = Image.open(path).convert("L"). Luego, debe cambiarse la dimensión de entrada de la MLP: input_size= 64 * 64 * 1= 4096, donde 1 representa la cantidad de canales. De esta manera, disminuye el número de parámetros que considera la red.

## 7. Regularización
- ¿Qué es la regularización en el contexto del entrenamiento de redes neuronales?
La regularización es el conjunto de técnicas que permite reducir el overfitting a través del ajuste de la cantidad de parámetros, para mejorar la generalización del modelo. Suele utilizarse cuando se tienen pocos datos para el entrenamiento de la red. 
- ¿Cuál es la diferencia entre `Dropout` y regularización `L2` (weight decay)?
El Dropout y la regularización L2 se diferencian en la forma en la que reducen el overfitting. El Dropout se basa una regularización estocástica: durante cada iteración de entrenamiento, apaga de forma aleatoria un porcentaje de neuronas. Al eliminar temporalmente algunas neuronas, la red se ve obligada a no depender de conexiones específicas y por ende, aprender otras nuevas, modificando los pesos de neuronas que antes eran muy bajos. De esta manera, evita la especialización.
Por otro lado, la regularización L2 funciona penalizando los pesos grandes del modelo. Para ello, modifica la función de pérdida (loss) agregando un término adicional proporcional al cuadrado de los pesos. Esto provoca que, durante el entrenamiento, los pesos tiendan a mantenerse pequeños, lo que ayuda a que el modelo generalice mejor. Es una técnica determinista: siempre actúa de la misma manera y no altera la estructura de la red, solo influye en la magnitud de los parámetros.
- ¿Qué es `BatchNorm` y cómo ayuda a estabilizar el entrenamiento?
BatchNorm (Batch Normalization) es una técnica que se aplica dentro de una red neuronal para normalizar las activaciones de cada capa durante el entrenamiento. Esto lo lleva a cabo por mini-batch, es decir, calcula la media y varianza de las activaciones por mini-batch; y luego, las normaliza para que tengan media 0 y varianza 1. Como no utiliza el total de observaciones para promediar, el cálculo es ruidoso. Esto tiene un efecto de regularización (cada nuevo batch se mueve lentamente con respecto al anterior) y en consecuencia, el entrenamiento resulta más estable. 
- ¿Cómo se relaciona `BatchNorm` con la velocidad de convergencia?
BatchNorm mejora la velocidad de convergencia porque estabiliza la distribución de las activaciones que recibe cada capa durante el entrenamiento. Cuando las activaciones cambian demasiado entre iteraciones, las capas deben reajustarse constantemente para adaptarse a esas variaciones; lo que hace que el entrenamiento sea más lento y menos eficiente. Al normalizar las activaciones usando la media y la varianza del mini-batch, las distribuciones resultan más controladas. Como consecuencia, el gradiente se vuelve más estable, por lo que puede avanzar más rápido hacia un mínimo en la loss.
- ¿Puede `BatchNorm` actuar como regularizador? ¿Por qué?
Si, como se mencionó anteriormente, la introducción de ruido por la normalización a partir del cálculo de la media y varianza por mini-batch dificulta que el modelo memorice el conjunto de datos de entrenamiento y por ende, reduce el riesgo de overfitting. Sin embargo, el Batch Normalization, estrictamente hablando, es una técnica de aceleración y no de regularización. 
- ¿Qué efectos visuales podrías observar en TensorBoard si hay overfitting?
El overfitting puede verse en las curvas de accuracy y loss en función de las épocas, durante el entrenamiento y validación. Si hay overfitting, la loss de entrenamiento disminuye de forma estable, pero la loss de validación llega a un mínimo y luego, empieza a aumentar. Asimismo, el accuracy en el entrenamiento alcanza valores muy altos, mientras que durante la validación, se estanca o disminuye. En ambos casos, puede verse una gran brecha valor a valor entre las curvas. 
- ¿Cómo ayuda la regularización a mejorar la generalización del modelo?
La regularización mejora la generalización porque evita que el modelo aprenda patrones demasiado específicos de los datos de entrenamiento, lo que suele suceder cuando tiene muchos parámetros o pocso datos. Las técnicas de regularización como L2 (que penaliza pesos grandes), Dropout (que apaga neuronas aleatoriamente), Early Stopping o DataAugmentation introducen restricciones controladas que fuerzan al modelo a depender menos de combinaciones específicas de neuronas, para generar nuevos caminos de conexiones. 

## 8. Inicialización de Parámetros
- ¿Por qué es importante la inicialización de los pesos en una red neuronal?
La inicialización de los pesos en una red neuronal es importante porque influye directamente en la estabilidad del entrenamiento, la velocidad de convergencia y la capacidad de la red para aprender. Una mala inicialización puede provocar que las activaciones y los gradientes se vuelvan demasiado grandes (explosión) o demasiado pequeños (desvanecimiento). Si los gradientes se vuelven muy pequeños, la red aprende extremadamente lento o deja de aprender; si son demasiado grandes, las actualizaciones pueden volverse inestables y el entrenamiento diverge. Así, una buena inicialización ayuda a mantener los valores controlados. 
- ¿Qué podría ocurrir si todos los pesos se inicializan con el mismo valor?
Si los pesos de una capa se inicializan todos iguales, las neuronas dentro de esa capa pueden terminar aprendiendo lo mismo, lo que se conoce como problema de simetría. En este caso, se disminuyen los grados de libertad y las neuronas resultan indistinguibles entre sí. Para romper esta simetría, es necesario que los pesos comiencen con valores distintos, de modo que cada neurona pueda aprender funciones diferentes.
- ¿Cuál es la diferencia entre las inicializaciones de Xavier (Glorot) y He?
La inicialización de Xavier (Glorot) tiene como objetivo mantener la varianza de las activaciones igual entre capas (no explotan ni se desvanecen). Así, elige pesos para mantener una varianza estable en la entrada y salida de la capa. Se utiliza para activaciones como Tanh o Sigmoidea, en las que no se eliminan valores. Por su parte, la inicialización He aumenta la varianza inicial de los pesos para compensar el hecho de que las activaciones ReLU y sus variantes descartan aproximadamente la mitad de sus valores (las entradas negativas). Al incrementar esa varianza, se evita que las activaciones y los gradientes se vuelvan demasiado pequeños a lo largo de la red.
- ¿Por qué en una red con ReLU suele usarse la inicialización de He?
Como se mencionó anteriormente, en una red con ReLU se eliminan aproximadamente la mitad de sus entradas: todos los valores negativos se convierten en 0. Eso significa que, después de pasar por una capa ReLU, la varianza de las activaciones disminuye. Si no se compensa esta pérdida, las activaciones y los gradientes pueden volverse demasiado pequeños (vanishing gradient) a medida que avanzan por la red, lo que lleva a un aprendizaje lento o incluso a que la red deje de aprender. La inicialización de He fue diseñada específicamente para aumentar la varianza inicial de los pesos usando una fórmula que considera solo el número de unidades de entrada y así, compensar la pérdida.
- ¿Qué capas de una red requieren inicialización explícita y cuáles no?
Las capas de una red que requieren inicialización explícita son las densas (Linear, presente en el pipeline) y las convolucionales. Esto se debe a que presentan pesos entrenables. Luego, las capas que no requieren inicialización son aquellas que no tienen pesos entrenables: como ReLU, que aplica una transformación matemática sin parámetros. Esto también aplica a operaciones estructurales como Dropout y Flatten. 

